{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyze MIBI-TOF image data\n\nThis tutorial shows how to apply Squidpy to MIBI-TOF data.\n\nThe data used here comes from a recent paper from\n`hartmann2020multiplexed`. We provide a pre-processed subset of the\ndata, in `anndata.AnnData` format. For details on how it was\npre-processed, please refer to the original paper.\n\n::: seealso\nSee `sphx_glr_auto_tutorials_tutorial_visium_hne.py` for additional\nanalysis using images and `sphx_glr_auto_tutorials_tutorial_seqfish.py`\nfor analysis using spatial graph functions.\n:::\n\n## Import packages & data\n\nTo run the notebook locally, create a conda environment as *conda env\ncreate -f environment.yml* using this\n[environment.yml](https://github.com/scverse/squidpy_notebooks/blob/main/environment.yml).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import squidpy as sq\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nadata = sq.datasets.mibitof()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The subset of the data we consider here comprises three biopsies\ncolorectal carcinoma biopsies from different donors, where MIBI-TOF was\nused to measure single-cell metabolic profiles. As imaging information,\nwe included three raw image channels:\n\n> -   [145_CD45]{.title-ref} - a immune cell marker (cyan).\n> -   [174_CK]{.title-ref} - a tumor marker (magenta).\n> -   [113_vimentin]{.title-ref} - a mesenchymal cell marker (yellow).\n\nand a cell segmentation mask provided by the authors of the original\npaper.\n\nThe [adata]{.title-ref} object contains three different libraries, one\nfor each biopsy. The images are contained in\n`adata.uns['spatial'][<library_id>]['images']`. Let us visualize the\ncluster annotations for each library using `squidpy.pl.spatial_scatter`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.pl.spatial_segment(adata, color=\"Cluster\", library_key=\"library_id\", seg_cell_id=\"cell_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us create an ImageContainer from the images contained in\n[adata]{.title-ref}. As all three biopsies are already joined in\n[adata]{.title-ref}, let us also create one ImageContainer for all three\nbiopsies using a z-stack. For more information on how to use\n[ImageContainer]{.title-ref} with z-stacks, also have a look at\n`sphx_glr_auto_tutorials_tutorial_image_container_zstacks.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "imgs = []\nfor library_id in adata.uns[\"spatial\"].keys():\n    img = sq.im.ImageContainer(adata.uns[\"spatial\"][library_id][\"images\"][\"hires\"], library_id=library_id)\n    img.add_img(adata.uns[\"spatial\"][library_id][\"images\"][\"segmentation\"], library_id=library_id, layer=\"segmentation\")\n    img[\"segmentation\"].attrs[\"segmentation\"] = True\n    imgs.append(img)\nimg = sq.im.ImageContainer.concat(imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we also added the segmentation as an additional layer to\n[img]{.title-ref}, and set the [segmentation]{.title-ref} attribute in\nthe ImageContainer. This allows visualization of the segmentation layer\nas a [labels]{.title-ref} layer in Napari.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have Napari installed, you can have a look at the data using the\ninteractive viewer: Note that you can load the segmentation layer as an\noverlay over the image.\n\n``` python\nimg.interactive(adata, library_key='library_id')\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us also statically visualize the data in [img]{.title-ref}, using\n`squidpy.im.ImageCntainer.show`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img.show(\"image\")\nimg.show(\"image\", segmentation_layer=\"segmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following we show how to use Squidpy to extract cellular mean\nintensity information using raw images and a provided segmentation mask.\nIn the present case, [adata]{.title-ref} of course already contains the\npost-processed cellular mean intensity for the raw image channels. The\naim of this tutorial, however, is to showcase how the extraction of such\nfeatures is possible using Squidpy. As Squidpy is backed by `dask` and\nsupports chunked image processing, also large images can be processed in\nthis way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert image to CMYK\n\nAs already mentioned, the images contain information from three raw\nchannels, [145_CD45]{.title-ref}, [174_CK]{.title-ref}, and\n[113_vimentin]{.title-ref}. As the channel information is encoded in\nCMYK space, we first need to convert the RGB images to CMYK.\n\nFor this, we can use `squidpy.im.ImageContainer.apply`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def rgb2cmyk(arr):\n    \"\"\"Convert arr from RGB to CMYK color space.\"\"\"\n    R = arr[..., 0] / 255\n    G = arr[..., 1] / 255\n    B = arr[..., 2] / 255\n    K = 1 - (np.max(arr, axis=-1) / 255)\n    C = (1 - R - K) / (1 - K + np.finfo(float).eps)  # avoid division by 0\n    M = (1 - G - K) / (1 - K + np.finfo(float).eps)\n    Y = (1 - B - K) / (1 - K + np.finfo(float).eps)\n    return np.stack([C, M, Y, K], axis=3)\n\n\nimg.apply(rgb2cmyk, layer=\"image\", new_layer=\"image_cmyk\", copy=False)\nimg.show(\"image_cmyk\", channelwise=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract per-cell mean intensity\n\nNow that we have disentangled the individual channels, let use use the\nprovided segmentation mask to extract per-cell mean intensities.\n\nBy default, the [segmentation]{.title-ref} feature extractor extracts\ninformation using all segments (cells) in the current crop. As we would\nlike to only get information of the segment (cell) in the center of the\ncurrent crop, let us use a [custom]{.title-ref} feature extractor.\n\nFist, define a custom feature extraction function. This function needs\nto get the segmentation mask and the original image as input. We will\nachieve this by passing an `additional_layers` argument to the\n[custom]{.title-ref} feature extractor. This special argument will pass\nthe values of every layer in [additional_layers]{.title-ref} to the\ncustom feature extraction function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def segmentation_image_intensity(arr, image_cmyk):\n    \"\"\"\n    Calculate per-channel mean intensity of the center segment.\n\n    arr: the segmentation\n    image_cmyk: the raw image values\n    \"\"\"\n    import skimage.measure\n\n    # the center of the segmentation mask contains the current label\n    # use that to calculate the mask\n    s = arr.shape[0]\n    mask = (arr == arr[s // 2, s // 2, 0, 0]).astype(int)\n    # use skimage.measure.regionprops to get the intensity per channel\n    features = []\n    for c in range(image_cmyk.shape[-1]):\n        feature = skimage.measure.regionprops_table(\n            np.squeeze(mask),  # skimage needs 3d or 2d images, so squeeze excess dims\n            intensity_image=np.squeeze(image_cmyk[:, :, :, c]),\n            properties=[\"mean_intensity\"],\n        )[\"mean_intensity\"][0]\n        features.append(feature)\n    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, use `squidpy.im.calculate_image_features` with the\n[custom]{.title-ref} feature extractor, specifying the function (`func`)\nto use, and the additional layers (`additional_layers`) to pass to the\nfunction. We will use `spot_scale = 10` to ensure that we also cover big\nsegments fully by one crop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.im.calculate_image_features(\n    adata,\n    img,\n    library_id=\"library_id\",\n    features=\"custom\",\n    spot_scale=10,\n    layer=\"segmentation\",\n    features_kwargs={\"custom\": {\"func\": segmentation_image_intensity, \"additional_layers\": [\"image_cmyk\"]}},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting features are stored in `adata.obs['img_features']`, with\nchannel 0 representing [145_CD45]{.title-ref}, channel 1\n[174_CK]{.title-ref}, and channel 2 [113_vimentin]{.title-ref}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adata.obsm[\"img_features\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As described in `hartmann2020multiplexed`, let us transformed using an\ninverse hyperbolic sine ([arcsinh]{.title-ref}) co-factor of 0.05, to\nallow us to compare the computed mean intensities with the values\ncontained in [adata]{.title-ref}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adata.obsm[\"img_features_transformed\"] = np.arcsinh(adata.obsm[\"img_features\"] / 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let\\'s visualize the result:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "channels = [\"CD45\", \"CK\", \"vimentin\"]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\nfor i, ax in enumerate(axes):\n    X = np.array(adata[:, channels[i]].X.todense())[:, 0]\n    Y = adata.obsm[\"img_features_transformed\"][f\"segmentation_image_intensity_{i}\"]\n    ax.scatter(X, Y)\n    ax.set_xlabel(\"true value in adata.X\")\n    ax.set_ylabel(\"computed mean intensity\")\n    corr = np.corrcoef(X, Y)[1, 0]\n    ax.set_title(f\"{channels[i]}, corr: {corr:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get high correlations between the original values and our computation\nusing Squidpy. The remaining differences are probably due to more\npre-processing applied by the authors of `hartmann2020multiplexed`.\n\nIn this tutorial we have shown how to pre-process imaging data to\nextract per-cell counts / mean intensities using Squidpy. Of course it\nis also possible to apply spatial statistics functions provided by the\n`squidpy.gr` module to MIBI-TOF data. For examples of this, please see\nour other Analysis tutorials, e.g.\n`sphx_glr_auto_tutorials_tutorial_seqfish.py`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}