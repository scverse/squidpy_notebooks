{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyze Visium fluorescence data\n\nThis tutorial shows how to apply Squidpy image analysis features for the\nanalysis of Visium data.\n\nFor a tutorial using Visium data that includes the graph analysis\nfunctions, have a look at\n`sphx_glr_auto_tutorials_tutorial_visium_hne.py`. The dataset used here\nconsists of a Visium slide of a coronal section of the mouse brain. The\noriginal dataset is publicly available at the 10x genomics [dataset\nportal](https://support.10xgenomics.com/spatial-gene-expression/datasets)\n. Here, we provide a pre-processed dataset, with pre-annotated clusters,\nin `anndata.AnnData` format and the tissue image in\n`squidpy.im.ImageContainer` format.\n\nA couple of notes on pre-processing:\n\n> -   The pre-processing pipeline is the same as the one shown in the\n>     original [Scanpy\n>     tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html)\n>     .\n> -   The cluster annotation was performed using several resources, such\n>     as the [Allen Brain\n>     Atlas](https://mouse.brain-map.org/experiment/thumbnails/100048576?image_type=atlas)\n>     , the [Mouse Brain gene expression atlas](http://mousebrain.org/)\n>     from the Linnarson lab and this recent pre-print `linnarson2020`.\n\n::: seealso\nSee `sphx_glr_auto_tutorials_tutorial_visium_hne.py` for additional\nanalysis examples.\n:::\n\n## Import packages & data\n\nTo run the notebook locally, create a conda environment as *conda env\ncreate -f environment.yml* using this\n[environment.yml](https://github.com/scverse/squidpy_notebooks/blob/main/environment.yml).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\nimport anndata as ad\nimport squidpy as sq\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nsc.logging.print_header()\nprint(f\"squidpy=={sq.__version__}\")\n\n# load the pre-processed dataset\nimg = sq.datasets.visium_fluo_image_crop()\nadata = sq.datasets.visium_fluo_adata_crop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let\\'s visualize the cluster annotation in the spatial context\nwith `squidpy.pl.spatial_scatter`.\n\nAs you can see, this dataset is a smaller crop of the whole brain\nsection. We provide this crop to make the execution time of this\ntutorial a bit shorter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.pl.spatial_scatter(adata, color=\"cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fluorescence image provided with this dataset has three channels:\n*DAPI* (specific to DNA), *anti-NEUN* (specific to neurons), *anti-GFAP*\n(specific to Glial cells). We can directly visualize the channels with\nthe method `squidpy.im.ImageContainer.show`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img.show(channelwise=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visium datasets contain high-resolution images of the tissue. Using the\nfunction `squidpy.im.calculate_image_features` you can calculate image\nfeatures for each Visium spot and create a `obs x features` matrix in\n`adata` that can then be analyzed together with the `obs x gene` gene\nexpression matrix.\n\nBy extracting image features we are aiming to get both similar and\ncomplementary information to the gene expression values. Similar\ninformation is for example present in the case of a tissue with two\ndifferent cell types whose morphology is different. Such cell type\ninformation is then contained in both the gene expression values and the\ntissue image features. Complementary or additional information is\npresent in the fact that we can use a nucleus segmentation to count\ncells and add features summarizing the immediate spatial neighborhood of\na spot.\n\nSquidpy contains several feature extractors and a flexible pipeline of\ncalculating features of different scales and sizes. There are several\ndetailed examples of how to use `squidpy.im.calculate_image_features`.\n`sphx_glr_auto_examples_image_compute_features.py` provides a good\nstarting point for learning more.\n\nHere, we will extract [summary]{.title-ref}, [histogram]{.title-ref},\n[segmentation]{.title-ref}, and [texture]{.title-ref} features. To\nprovide more context and allow the calculation of multi-scale features,\nwe will additionally calculate [summary]{.title-ref} and\n[histogram]{.title-ref} features at different crop sizes and scales.\n\n# Image segmentation\n\nTo calculate [segmentation]{.title-ref} features, we first need to\nsegment the tissue image using `squidpy.im.segment`. But even before\nthat, it\\'s best practice to pre-process the image by e.g. smoothing it\nusing in `squidpy.im.process`. We will then use the *DAPI* channel of\nthe fluorescence image (`channel_id s= 0`). Please refer to\n`sphx_glr_auto_examples_image_compute_segment_fluo.py` for more details\non how to calculate a segmented image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.im.process(\n    img=img,\n    layer=\"image\",\n    method=\"smooth\",\n)\n\nsq.im.segment(img=img, layer=\"image_smooth\", method=\"watershed\", channel=0, chunks=1000)\n\n# plot the resulting segmentation\nfig, ax = plt.subplots(1, 2)\nimg_crop = img.crop_corner(2000, 2000, size=500)\nimg_crop.show(layer=\"image\", channel=0, ax=ax[0])\nimg_crop.show(\n    layer=\"segmented_watershed\",\n    channel=0,\n    ax=ax[1],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result of `squidpy.im.segment` is saved in\n`img['segmented_watershed']` by default. It is a label image where each\nsegmented object is annotated with a different integer number.\n\n# Segmentation features\n\nWe can now use the segmentation to calculate segmentation features.\nThese include morphological features of the segmented objects and\nchannel-wise image intensities beneath the segmentation mask. In\nparticular, we can count the segmented objects within each Visium spot\nto get an approximation of the number of cells. In addition, we can\ncalculate the mean intensity of each fluorescence channel within the\nsegmented objects. Depending on the fluorescence channels, this can give\nus e.g., an estimation of the cell type. For more details on how the\nsegmentation features, you can have a look at the docs of\n`squidpy.im.calculate_image_features` or the example at\n`sphx_glr_auto_examples_image_compute_segmentation_features.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define image layer to use for segmentation\nfeatures_kwargs = {\"segmentation\": {\"label_layer\": \"segmented_watershed\"}}\n# calculate segmentation features\nsq.im.calculate_image_features(\n    adata,\n    img,\n    features=\"segmentation\",\n    layer=\"image\",\n    key_added=\"features_segmentation\",\n    n_jobs=1,\n    features_kwargs=features_kwargs,\n)\n# plot results and compare with gene-space clustering\nsq.pl.spatial_scatter(\n    sq.pl.extract(adata, \"features_segmentation\"),\n    color=[\n        \"segmentation_label\",\n        \"cluster\",\n        \"segmentation_ch-0_mean_intensity_mean\",\n        \"segmentation_ch-1_mean_intensity_mean\",\n    ],\n    frameon=False,\n    ncols=2,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we made use of `squidpy.pl.extract`, a method to extract all\nfeatures in a given [adata.obsm\\[\\'{key}\\'\\]]{.title-ref} and\ntemporarily save them to `anndata.AnnData.obs`. Such method is\nparticularly useful for plotting purpose, as shown above.\n\nThe number of cells per Visium spot provides an interesting view of the\ndata that can enhance the characterization of gene-space clusters. We\ncan see that the cell-rich pyramidal layer of the Hippocampus has more\ncells than the surrounding areas (upper left). This fine-grained view of\nthe Hippocampus is not visible in the gene clusters where the\nHippocampus is one cluster only.\n\nThe per-channel intensities plotted in the second row show us that the\nareas labeled with *Cortex_1* and *Cortex_3* have a higher intensity of\nchannel 1, *anti-NEUN* (lower left). This means that these areas have\nmore neurons that the remaining areas in this crop. In addition, cluster\n*Fiber_tracts* and *lateral ventricles* seems to be enriched with *Glial\ncells*, seen by the larger mean intensities of channel 2, *anti-GFAP*,\nin these areas (lower right).\n\n# Extract and cluster features\n\nNow we will calculate summary, histogram, and texture features. These\nfeatures provide a useful compressed summary of the tissue image. For\nmore information on these features, refer to:\n\n> -   `sphx_glr_auto_examples_image_compute_summary_features.py`.\n> -   `sphx_glr_auto_examples_image_compute_histogram_features.py`.\n> -   `sphx_glr_auto_examples_image_compute_texture_features.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define different feature calculation combinations\nparams = {\n    # all features, corresponding only to tissue underneath spot\n    \"features_orig\": {\n        \"features\": [\"summary\", \"texture\", \"histogram\"],\n        \"scale\": 1.0,\n        \"mask_circle\": True,\n    },\n    # summary and histogram features with a bit more context, original resolution\n    \"features_context\": {\"features\": [\"summary\", \"histogram\"], \"scale\": 1.0},\n    # summary and histogram features with more context and at lower resolution\n    \"features_lowres\": {\"features\": [\"summary\", \"histogram\"], \"scale\": 0.25},\n}\n\nfor feature_name, cur_params in params.items():\n    # features will be saved in `adata.obsm[feature_name]`\n    sq.im.calculate_image_features(adata, img, layer=\"image\", key_added=feature_name, n_jobs=1, **cur_params)\n\n# combine features in one dataframe\nadata.obsm[\"features\"] = pd.concat([adata.obsm[f] for f in params.keys()], axis=\"columns\")\n\n# make sure that we have no duplicated feature names in the combined table\nadata.obsm[\"features\"].columns = ad.utils.make_index_unique(adata.obsm[\"features\"].columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the extracted image features to compute a new cluster\nannotation. This could be useful to gain insights in similarities across\nspots based on image morphology.\n\nFor this, we first define a helper function to cluster features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cluster_features(features: pd.DataFrame, like=None):\n    \"\"\"\n    Calculate leiden clustering of features.\n\n    Specify filter of features using `like`.\n    \"\"\"\n    # filter features\n    if like is not None:\n        features = features.filter(like=like)\n    # create temporary adata to calculate the clustering\n    adata = ad.AnnData(features)\n    # important - feature values are not scaled, so need to scale them before PCA\n    sc.pp.scale(adata)\n    # calculate leiden clustering\n    sc.pp.pca(adata, n_comps=min(10, features.shape[1] - 1))\n    sc.pp.neighbors(adata)\n    sc.tl.leiden(adata)\n\n    return adata.obs[\"leiden\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we calculate feature clusters using different features and compare\nthem to gene clusters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adata.obs[\"features_summary_cluster\"] = cluster_features(adata.obsm[\"features\"], like=\"summary\")\nadata.obs[\"features_histogram_cluster\"] = cluster_features(adata.obsm[\"features\"], like=\"histogram\")\nadata.obs[\"features_texture_cluster\"] = cluster_features(adata.obsm[\"features\"], like=\"texture\")\n\nsc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\nsq.pl.spatial_scatter(\n    adata,\n    color=[\n        \"features_summary_cluster\",\n        \"features_histogram_cluster\",\n        \"features_texture_cluster\",\n        \"cluster\",\n    ],\n    ncols=3,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like the gene-space clusters (bottom middle), the feature space clusters\nare also spatially coherent.\n\nThe feature clusters of the different feature extractors are quite\ndiverse, but all of them reflect the structure of the Hippocampus by\nassigning different clusters to the different structural areas. This is\na higher level of detail than the gene-space clustering provides with\nonly one cluster for the Hippocampus.\n\nThe feature clusters also show the layered structure of the cortex, but\nagain subdividing it in more clusters than the gene-space clustering. It\nmight be possible to re-cluster the gene expression counts with a higher\nresolution to also get more fine-grained clusters, but nevertheless the\nimage features seem to provide additional supporting information to the\ngene-space clusters.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}