{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyze Visium H&E data\n\nThis tutorial shows how to apply Squidpy for the analysis of Visium\nspatial transcriptomics data.\n\nThe dataset used here consists of a Visium slide of a coronal section of\nthe mouse brain. The original dataset is publicly available at the 10x\nGenomics [dataset\nportal](https://support.10xgenomics.com/spatial-gene-expression/datasets)\n. Here, we provide a pre-processed dataset, with pre-annotated clusters,\nin AnnData format and the tissue image in `squidpy.im.ImageContainer`\nformat.\n\nA couple of notes on pre-processing:\n\n> -   The pre-processing pipeline is the same as the one shown in the\n>     original [Scanpy\n>     tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html)\n>     .\n> -   The cluster annotation was performed using several resources, such\n>     as the [Allen Brain\n>     Atlas](https://mouse.brain-map.org/experiment/thumbnails/100048576?image_type=atlas)\n>     , the [Mouse Brain gene expression atlas](http://mousebrain.org/)\n>     from the Linnarson lab and this recent\n>     [pre-print](https://www.biorxiv.org/content/10.1101/2020.07.24.219758v1)\n>     .\n\n::: seealso\nSee `sphx_glr_auto_tutorials_tutorial_visium_fluo.py` for a detailed\nanalysis example of image features.\n:::\n\n## Import packages & data\n\nTo run the notebook locally, create a conda environment as *conda env\ncreate -f environment.yml* using this\n[environment.yml](https://github.com/scverse/squidpy_notebooks/blob/main/environment.yml).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\nimport anndata as ad\nimport squidpy as sq\n\nimport numpy as np\nimport pandas as pd\n\nsc.logging.print_header()\nprint(f\"squidpy=={sq.__version__}\")\n\n# load the pre-processed dataset\nimg = sq.datasets.visium_hne_image()\nadata = sq.datasets.visium_hne_adata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let\\'s visualize cluster annotation in spatial context with\n`squidpy.pl.spatial_scatter`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.pl.spatial_scatter(adata, color=\"cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image features\n\nVisium datasets contain high-resolution images of the tissue that was\nused for the gene extraction. Using the function\n`squidpy.im.calculate_image_features` you can calculate image features\nfor each Visium spot and create a `obs x features` matrix in `adata`\nthat can then be analyzed together with the `obs x gene` gene expression\nmatrix.\n\nBy extracting image features we are aiming to get both similar and\ncomplementary information to the gene expression values. Similar\ninformation is for example present in the case of a tissue with two\ndifferent cell types whose morphology is different. Such cell type\ninformation is then contained in both the gene expression values and the\ntissue image features.\n\nSquidpy contains several feature extractors and a flexible pipeline of\ncalculating features of different scales and sizes. There are several\ndetailed examples of how to use `squidpy.im.calculate_image_features`.\n`sphx_glr_auto_examples_image_compute_features.py` provides a good\nstarting point for learning more.\n\nHere, we will extract [summary]{.title-ref} features at different crop\nsizes and scales to allow the calculation of multi-scale features and\n[segmentation]{.title-ref} features. For more information on the summary\nfeatures, also refer to\n`sphx_glr_auto_examples_image_compute_summary_features.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# calculate features for different scales (higher value means more context)\nfor scale in [1.0, 2.0]:\n    feature_name = f\"features_summary_scale{scale}\"\n    sq.im.calculate_image_features(\n        adata,\n        img.compute(),\n        features=\"summary\",\n        key_added=feature_name,\n        n_jobs=4,\n        scale=scale,\n    )\n\n\n# combine features in one dataframe\nadata.obsm[\"features\"] = pd.concat(\n    [adata.obsm[f] for f in adata.obsm.keys() if \"features_summary\" in f], axis=\"columns\"\n)\n# make sure that we have no duplicated feature names in the combined table\nadata.obsm[\"features\"].columns = ad.utils.make_index_unique(adata.obsm[\"features\"].columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the extracted image features to compute a new cluster\nannotation. This could be useful to gain insights in similarities across\nspots based on image morphology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helper function returning a clustering\ndef cluster_features(features: pd.DataFrame, like=None) -> pd.Series:\n    \"\"\"\n    Calculate leiden clustering of features.\n\n    Specify filter of features using `like`.\n    \"\"\"\n    # filter features\n    if like is not None:\n        features = features.filter(like=like)\n    # create temporary adata to calculate the clustering\n    adata = ad.AnnData(features)\n    # important - feature values are not scaled, so need to scale them before PCA\n    sc.pp.scale(adata)\n    # calculate leiden clustering\n    sc.pp.pca(adata, n_comps=min(10, features.shape[1] - 1))\n    sc.pp.neighbors(adata)\n    sc.tl.leiden(adata)\n\n    return adata.obs[\"leiden\"]\n\n\n# calculate feature clusters\nadata.obs[\"features_cluster\"] = cluster_features(adata.obsm[\"features\"], like=\"summary\")\n\n# compare feature and gene clusters\nsq.pl.spatial_scatter(adata, color=[\"features_cluster\", \"cluster\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing gene and feature clusters, we notice that in some regions,\nthey look very similar, like the cluster *Fiber_tract*, or clusters\naround the Hippocampus seems to be roughly recapitulated by the clusters\nin image feature space. In others, the feature clusters look different,\nlike in the cortex, where the gene clusters show the layered structure\nof the cortex, and the features clusters rather seem to show different\nregions of the cortex.\n\nThis is only a simple, comparative analysis of the image features, note\nthat you could also use the image features to e.g. compute a common\nimage and gene clustering by computing a shared neighbors graph (for\ninstance on concatenated PCAs on both feature spaces).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spatial statistics and graph analysis\n\nSimilar to other spatial data, we can investigate spatial organization\nby leveraging spatial and graph statistics in Visium data.\n\n## Neighborhood enrichment\n\nComputing a neighborhood enrichment can help us identify spots clusters\nthat share a common neighborhood structure across the tissue. We can\ncompute such score with the following function:\n`squidpy.gr.nhood_enrichment`. In short, it\\'s an enrichment score on\nspatial proximity of clusters: if spots belonging to two different\nclusters are often close to each other, then they will have a high score\nand can be defined as being *enriched*. On the other hand, if they are\nfar apart, and therefore are seldom a neighborhood, the score will be\nlow and they can be defined as *depleted*. This score is based on a\npermutation-based test, and you can set the number of permutations with\nthe `n_perms` argument (default is 1000).\n\nSince the function works on a connectivity matrix, we need to compute\nthat as well. This can be done with `squidpy.gr.spatial_neighbors`.\nPlease see `sphx_glr_auto_examples_graph_compute_spatial_neighbors.py`\nfor more details of how this function works.\n\nFinally, we\\'ll directly visualize the results with\n`squidpy.pl.nhood_enrichment`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.gr.spatial_neighbors(adata)\nsq.gr.nhood_enrichment(adata, cluster_key=\"cluster\")\nsq.pl.nhood_enrichment(adata, cluster_key=\"cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the spatial organization of the mouse brain coronal section, not\nsurprisingly we find high neighborhood enrichment the Hippocampus\nregion: *Pyramidal_layer_dentate_gyrus* and *Pyramidal_layer* clusters\nseems to be often neighbors with the larger *Hippocampus* cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Co-occurrence across spatial dimensions\n\nIn addition to the neighbor enrichment score, we can visualize cluster\nco-occurrence in spatial dimensions. This is a similar analysis of the\none presented above, yet it does not operate on the connectivity matrix,\nbut on the original spatial coordinates. The co-occurrence score is\ndefined as:\n\n$$\\frac{p(exp|cond)}{p(exp)}$$\n\nwhere $p(exp|cond)$ is the conditional probability of observing a\ncluster $exp$ conditioned on the presence of a cluster $cond$, whereas\n$p(exp)$ is the probability of observing $exp$ in the radius size of\ninterest. The score is computed across increasing radii size around each\nobservation (i.e. spots here) in the tissue.\n\nWe are gonna compute such score with `squidpy.gr.co_occurrence` and set\nthe cluster annotation for the conditional probability with the argument\n`clusters`. Then, we visualize the results with\n`squidpy.pl.co_occurrence`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.gr.co_occurrence(adata, cluster_key=\"cluster\")\nsq.pl.co_occurrence(\n    adata,\n    cluster_key=\"cluster\",\n    clusters=\"Hippocampus\",\n    figsize=(8, 4),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result largely recapitulates the previous analysis: the\n*Pyramidal_layer* cluster seem to co-occur at short distances with the\nlarger *Hippocampus* cluster. It should be noted that the distance units\nare given in pixels of the Visium `source_image`, and corresponds to the\nsame unit of the spatial coordinates saved in `adata.obsm['spatial']`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ligand-receptor interaction analysis\n\nWe are continuing the analysis showing couple of feature-level methods\nthat are very relevant for the analysis of spatial molecular data. For\ninstance, after quantification of cluster co-occurrence, we might be\ninterested in finding molecular instances that could potentially drive\ncellular communication. This naturally translates in a ligand-receptor\ninteraction analysis. In Squidpy, we provide a fast re-implementation\nthe popular method CellPhoneDB `cellphonedb`\n([code](https://github.com/Teichlab/cellphonedb) ) and extended its\ndatabase of annotated ligand-receptor interaction pairs with the popular\ndatabase *Omnipath* `omnipath`. You can run the analysis for all\nclusters pairs, and all genes (in seconds, without leaving this\nnotebook), with `squidpy.gr.ligrec`. Furthermore, we\\'ll directly\nvisualize the results, filtering out lowly-expressed genes (with the\n`means_range` argument) and increasing the threshold for the adjusted\np-value (with the `alpha` argument). We\\'ll also subset the\nvisualization for only one source group, the *Hippocampus* cluster, and\ntwo target groups, *Pyramidal_layer_dentate_gyrus* and *Pyramidal_layer*\ncluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.gr.ligrec(\n    adata,\n    n_perms=100,\n    cluster_key=\"cluster\",\n)\nsq.pl.ligrec(\n    adata,\n    cluster_key=\"cluster\",\n    source_groups=\"Hippocampus\",\n    target_groups=[\"Pyramidal_layer\", \"Pyramidal_layer_dentate_gyrus\"],\n    means_range=(3, np.inf),\n    alpha=1e-4,\n    swap_axes=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dotplot visualization provides an interesting set of candidate\nligand-receptor annotation that could be involved in cellular\ninteractions in the Hippocampus. A more refined analysis would be for\ninstance to integrate these results with the results of a deconvolution\nmethod, to understand what\\'s the proportion of single-cell cell types\npresent in this region of the tissue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spatially variable genes with Moran\\'s I\n\nFinally, we might be interested in finding genes that show spatial\npatterns. There are several methods that aimed at address this\nexplicitly, based on point processes or Gaussian process regression\nframework:\n\n> -   *SPARK* -\n>     [paper](https://www.nature.com/articles/s41592-019-0701-7#Abs1),\n>     [code](https://github.com/xzhoulab/SPARK).\n> -   *Spatial DE* -\n>     [paper](https://www.nature.com/articles/nmeth.4636),\n>     [code](https://github.com/Teichlab/SpatialDE).\n> -   *trendsceek* -\n>     [paper](https://www.nature.com/articles/nmeth.4634),\n>     [code](https://github.com/edsgard/trendsceek).\n> -   *HMRF* - [paper](https://www.nature.com/articles/nbt.4260),\n>     [code](https://bitbucket.org/qzhudfci/smfishhmrf-py).\n\nHere, we provide a simple approach based on the well-known [Moran\\'s I\nstatistics](https://en.wikipedia.org/wiki/Moran%27s_I) which is in fact\nused also as a baseline method in the spatially variable gene papers\nlisted above. The function in Squidpy is called\n`squidpy.gr.spatial_autocorr`, and returns both test statistics and\nadjusted p-values in `anndata.AnnData.var` slot. For time reasons, we\nwill evaluate a subset of the highly variable genes only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "genes = adata[:, adata.var.highly_variable].var_names.values[:1000]\nsq.gr.spatial_autocorr(\n    adata,\n    mode=\"moran\",\n    genes=genes,\n    n_perms=100,\n    n_jobs=1,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results are saved in `adata.uns['moranI']` slot. Genes have already\nbeen sorted by Moran\\'s I statistic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adata.uns[\"moranI\"].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can select few genes and visualize their expression levels in the\ntissue with `squidpy.pl.spatial_scatter`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sq.pl.spatial_scatter(adata, color=[\"Olfm1\", \"Plp1\", \"Itpka\", \"cluster\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, some of these genes seems to be related to the\n*pyramidal* layers and the *fiber tract*.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}