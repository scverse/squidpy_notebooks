
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_mibitof.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/theislab/squidpy_notebooks/master?filepath=docs/source/auto_tutorials/tutorial_mibitof.ipynb
      :alt: Launch binder
      :width: 150 px

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_mibitof.py:

Analyze MIBI-TOF image data
===========================

This tutorial shows how to apply Squidpy to MIBI-TOF data.

The data used here comes from a recent paper from :cite:`hartmann2020multiplexed`.
We provide a pre-processed subset of the data, in :class:`anndata.AnnData` format.
For details on how it was pre-processed, please refer to the original paper.

.. seealso::

    See :ref:`sphx_glr_auto_tutorials_tutorial_visium_hne.py` for additional analysis using images
    and :ref:`sphx_glr_auto_tutorials_tutorial_seqfish.py` for analysis using spatial graph functions.

Import packages & data
----------------------
To run the notebook locally, create a conda environment as *conda env create -f environment.yml* using this
`environment.yml <https://github.com/theislab/squidpy_notebooks/blob/master/environment.yml>`_

.. GENERATED FROM PYTHON SOURCE LINES 21-31

.. code-block:: default


    import scanpy as sc
    import squidpy as sq

    import numpy as np

    import matplotlib.pyplot as plt

    adata = sq.datasets.mibitof()








.. GENERATED FROM PYTHON SOURCE LINES 32-47

The subset of the data we consider here comprises three biopsies colorectal carcinoma biopsies
from different donors, where MIBI-TOF was used to measure single-cell metabolic profiles.
As imaging information, we included three raw image channels:

  - `145_CD45` - a immune cell marker (cyan).
  - `174_CK` - a tumor marker (magenta).
  - `113_vimentin` - a mesenchymal cell marker (yellow).

and a cell segmentation mask provided by the authors of the original paper.

The `adata` object contains three different libraries, one for each biopsy.
The images are contained in ``adata.uns['spatial'][<library_id>]['images']``.
Let us visualize the cluster annotations for each library using :func:`scanpy.pl.spatial`.
For this, we need to subset `adata` to the desired `library_id`, using the mapping from `obs`
to `library_id` provided by ``adata.obs['library_id']``.

.. GENERATED FROM PYTHON SOURCE LINES 47-53

.. code-block:: default


    for library_id in adata.uns["spatial"].keys():
        sc.pl.spatial(
            adata[adata.obs["library_id"] == library_id], color="Cluster", library_id=library_id, title=library_id
        )




.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /auto_tutorials/images/sphx_glr_tutorial_mibitof_001.png
          :alt: point16
          :class: sphx-glr-multi-img

    *

      .. image:: /auto_tutorials/images/sphx_glr_tutorial_mibitof_002.png
          :alt: point23
          :class: sphx-glr-multi-img

    *

      .. image:: /auto_tutorials/images/sphx_glr_tutorial_mibitof_003.png
          :alt: point8
          :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 54-59

Let us create an ImageContainer from the images contained in `adata`.
As all three biopsies are already joined in `adata`, let us also create one ImageContainer for
all three biopsies using a z-stack.
For more information on how to use `ImageContainer` with z-stacks, also have a look at
:ref:`sphx_glr_auto_tutorials_tutorial_image_container_zstacks.py`.

.. GENERATED FROM PYTHON SOURCE LINES 59-67

.. code-block:: default

    imgs = []
    for library_id in adata.uns["spatial"].keys():
        img = sq.im.ImageContainer(adata.uns["spatial"][library_id]["images"]["hires"], library_id=library_id)
        img.add_img(adata.uns["spatial"][library_id]["images"]["segmentation"], library_id=library_id, layer="segmentation")
        img["segmentation"].attrs["segmentation"] = True
        imgs.append(img)
    img = sq.im.ImageContainer.concat(imgs)








.. GENERATED FROM PYTHON SOURCE LINES 68-71

Note that we also added the segmentation as an additional layer to `img`, and set the
`segmentation` attribute in the ImageContainer.
This allows visualization of the segmentation layer as a `labels` layer in Napari.

.. GENERATED FROM PYTHON SOURCE LINES 71-73

.. code-block:: default

    img






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    ImageContainer object with 2 layers:<p style='text-indent: 25px; margin-top: 0px; margin-bottom: 0px;'><strong>image</strong>: <em>y</em> (1024), <em>x</em> (1024), <em>z</em> (3), <em>channels</em> (3)</p><p style='text-indent: 25px; margin-top: 0px; margin-bottom: 0px;'><strong>segmentation</strong>: <em>y</em> (1024), <em>x</em> (1024), <em>z</em> (3), <em>channels_0</em> (1)</p>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 74-80

If you have Napari installed, you can have a look at the data using the interactive viewer:
Note that you can load the segmentation layer as an overlay over the image.

.. code-block:: python

    img.interactive(adata, library_key='library_id')

.. GENERATED FROM PYTHON SOURCE LINES 82-83

Let us also statically visualize the data in `img`, using :func:`squidpy.im.ImageCntainer.show`:

.. GENERATED FROM PYTHON SOURCE LINES 83-86

.. code-block:: default

    img.show("image")
    img.show("image", segmentation_layer="segmentation")




.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /auto_tutorials/images/sphx_glr_tutorial_mibitof_004.png
          :alt: image, library_id:point16, image, library_id:point23, image, library_id:point8
          :class: sphx-glr-multi-img

    *

      .. image:: /auto_tutorials/images/sphx_glr_tutorial_mibitof_005.png
          :alt: image, library_id:point16, image, library_id:point23, image, library_id:point8
          :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 87-94

In the following we show how to use Squidpy to extract cellular mean intensity information using raw images
and a provided segmentation mask.
In the present case, `adata` of course already contains the post-processed cellular mean intensity
for the raw image channels.
The aim of this tutorial, however, is to showcase how the extraction of such features is possible using Squidpy.
As Squidpy is backed by :mod:`dask` and supports chunked image processing,
also large images can be processed in this way.

.. GENERATED FROM PYTHON SOURCE LINES 96-103

Convert image to CMYK
---------------------
As already mentioned, the images contain information from three raw channels, `145_CD45`,
`174_CK`, and `113_vimentin`.
As the channel information is encoded in CMYK space, we first need to convert the RGB images to CMYK.

For this, we can use :meth:`squidpy.im.ImageContainer.apply`.

.. GENERATED FROM PYTHON SOURCE LINES 103-120

.. code-block:: default



    def rgb2cmyk(arr):
        """Convert arr from RGB to CMYK color space."""
        R = arr[..., 0] / 255
        G = arr[..., 1] / 255
        B = arr[..., 2] / 255
        K = 1 - (np.max(arr, axis=-1) / 255)
        C = (1 - R - K) / (1 - K + np.finfo(float).eps)  # avoid division by 0
        M = (1 - G - K) / (1 - K + np.finfo(float).eps)
        Y = (1 - B - K) / (1 - K + np.finfo(float).eps)
        return np.stack([C, M, Y, K], axis=3)


    img.apply(rgb2cmyk, layer="image", new_layer="image_cmyk", copy=False)
    img.show("image_cmyk", channelwise=True)




.. image:: /auto_tutorials/images/sphx_glr_tutorial_mibitof_006.png
    :alt: image_cmyk:0, library_id:point16, image_cmyk:1, library_id:point16, image_cmyk:2, library_id:point16, image_cmyk:3, library_id:point16, image_cmyk:0, library_id:point23, image_cmyk:1, library_id:point23, image_cmyk:2, library_id:point23, image_cmyk:3, library_id:point23, image_cmyk:0, library_id:point8, image_cmyk:1, library_id:point8, image_cmyk:2, library_id:point8, image_cmyk:3, library_id:point8
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 121-136

Extract per-cell mean intensity
-------------------------------
Now that we have disentangled the individual channels, let use use the provided segmentation mask
to extract per-cell mean intensities.

By default, the `segmentation` feature extractor extracts information using all segments (cells)
in the current crop.
As we would like to only get information of the segment (cell) in the center of the current crop,
let us use a `custom` feature extractor.

Fist, define a custom feature extraction function. This function needs to get the segmentation mask
and the original image as input.
We will achieve this by passing an ``additional_layers`` argument to the `custom` feature extractor.
This special argument will pass the values of every layer in `additional_layers`
to the custom feature extraction function.

.. GENERATED FROM PYTHON SOURCE LINES 136-163

.. code-block:: default



    def segmentation_image_intensity(arr, image_cmyk):
        """
        Calculate per-channel mean intensity of the center segment.

        arr: the segmentation
        image_cmyk: the raw image values
        """
        import skimage.measure

        # the center of the segmentation mask contains the current label
        # use that to calculate the mask
        s = arr.shape[0]
        mask = (arr == arr[s // 2, s // 2, 0, 0]).astype(int)
        # use skimage.measure.regionprops to get the intensity per channel
        features = []
        for c in range(image_cmyk.shape[-1]):
            feature = skimage.measure.regionprops_table(
                np.squeeze(mask),  # skimage needs 3d or 2d images, so squeeze excess dims
                intensity_image=np.squeeze(image_cmyk[:, :, :, c]),
                properties=["mean_intensity"],
            )["mean_intensity"][0]
            features.append(feature)
        return features









.. GENERATED FROM PYTHON SOURCE LINES 164-168

Now, use :func:`squidpy.im.calculate_image_features` with the `custom` feature extractor,
specifying the function (``func``) to use, and the additional layers (``additional_layers``)
to pass to the function.
We will use ``spot_scale = 10`` to ensure that we also cover big segments fully by one crop.

.. GENERATED FROM PYTHON SOURCE LINES 168-178

.. code-block:: default

    sq.im.calculate_image_features(
        adata,
        img,
        library_id="library_id",
        features="custom",
        spot_scale=10,
        layer="segmentation",
        features_kwargs={"custom": {"func": segmentation_image_intensity, "additional_layers": ["image_cmyk"]}},
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/3309 [00:00<?, ?/s]




.. GENERATED FROM PYTHON SOURCE LINES 179-181

The resulting features are stored in ``adata.obs['img_features']``,
with channel 0 representing `145_CD45`, channel 1 `174_CK`, and channel 2 `113_vimentin`.

.. GENERATED FROM PYTHON SOURCE LINES 181-183

.. code-block:: default

    adata.obsm["img_features"]






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>segmentation_image_intensity_0</th>
          <th>segmentation_image_intensity_1</th>
          <th>segmentation_image_intensity_2</th>
          <th>segmentation_image_intensity_3</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>3034-0</th>
          <td>0.000000</td>
          <td>0.995041</td>
          <td>0.010664</td>
          <td>0.492503</td>
        </tr>
        <tr>
          <th>3035-0</th>
          <td>0.000049</td>
          <td>0.884839</td>
          <td>0.042991</td>
          <td>0.713101</td>
        </tr>
        <tr>
          <th>3036-0</th>
          <td>0.680350</td>
          <td>0.000235</td>
          <td>0.222640</td>
          <td>0.948284</td>
        </tr>
        <tr>
          <th>3037-0</th>
          <td>0.813055</td>
          <td>0.000000</td>
          <td>0.173941</td>
          <td>0.790169</td>
        </tr>
        <tr>
          <th>3038-0</th>
          <td>0.420203</td>
          <td>0.015063</td>
          <td>0.486171</td>
          <td>0.709584</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>47342-2</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.696113</td>
          <td>0.855720</td>
        </tr>
        <tr>
          <th>47343-2</th>
          <td>0.441017</td>
          <td>0.000000</td>
          <td>0.587986</td>
          <td>0.941870</td>
        </tr>
        <tr>
          <th>47344-2</th>
          <td>0.639157</td>
          <td>0.000000</td>
          <td>0.344870</td>
          <td>0.858989</td>
        </tr>
        <tr>
          <th>47345-2</th>
          <td>0.196760</td>
          <td>0.000000</td>
          <td>0.612479</td>
          <td>0.855991</td>
        </tr>
        <tr>
          <th>47346-2</th>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.774775</td>
          <td>0.981311</td>
        </tr>
      </tbody>
    </table>
    <p>3309 rows × 4 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 184-187

As described in :cite:`hartmann2020multiplexed`, let us transformed using an
inverse hyperbolic sine (`arcsinh`) co-factor of 0.05, to allow us to compare
the computed mean intensities with the values contained in `adata`.

.. GENERATED FROM PYTHON SOURCE LINES 187-189

.. code-block:: default

    adata.obsm["img_features_transformed"] = np.arcsinh(adata.obsm["img_features"] / 0.05)








.. GENERATED FROM PYTHON SOURCE LINES 190-191

Now, let's visualize the result:

.. GENERATED FROM PYTHON SOURCE LINES 191-203

.. code-block:: default

    channels = ["CD45", "CK", "vimentin"]

    fig, axes = plt.subplots(1, 3, figsize=(15, 3))
    for i, ax in enumerate(axes):
        X = np.array(adata[:, channels[i]].X.todense())[:, 0]
        Y = adata.obsm["img_features_transformed"][f"segmentation_image_intensity_{i}"]
        ax.scatter(X, Y)
        ax.set_xlabel("true value in adata.X")
        ax.set_ylabel("computed mean intensity")
        corr = np.corrcoef(X, Y)[1, 0]
        ax.set_title(f"{channels[i]}, corr: {corr:.2f}")




.. image:: /auto_tutorials/images/sphx_glr_tutorial_mibitof_007.png
    :alt: CD45, corr: 0.84, CK, corr: 0.85, vimentin, corr: 0.70
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 204-214

We get high correlations between the original values and our computation using Squidpy.
The remaining differences are probably due to more pre-processing applied by
the authors of :cite:`hartmann2020multiplexed`.

In this tutorial we have shown how to pre-process imaging data to extract per-cell
counts / mean intensities using Squidpy.
Of course it is also possible to apply spatial statistics functions provided by the
:mod:`squidpy.gr` module to MIBI-TOF data.
For examples of this, please see our other Analysis tutorials, e.g.
:ref:`sphx_glr_auto_tutorials_tutorial_seqfish.py`.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  10.349 seconds)

**Estimated memory usage:**  229 MB


.. _sphx_glr_download_auto_tutorials_tutorial_mibitof.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tutorial_mibitof.py <tutorial_mibitof.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tutorial_mibitof.ipynb <tutorial_mibitof.ipynb>`
